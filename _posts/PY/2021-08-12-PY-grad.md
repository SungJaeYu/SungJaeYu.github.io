---
title: "[Pytorch] Grad 관련"
---

# requires_grad
Tensor 생성 parameter에는 **requires_grad**가 존재한다.    
default값은 False로 이 값을 True로 할경우 grad를 구하게 된다.   
즉, 경사하강법에서 사용되는 weight의 경우는 grad를 구해야 하지만   
입력값은 고정된 값이므로 grad를 구할 필요가 없다.   
```
w = torch.randn(3, requires_grad = True)
```

이 grad 계산을 취소 또는 생략하는 방법은 아래와 같다.  
```
# Option 1
w.requires_grad_(False)
# Option 2
w.detach()
# Option 3
with torch.no_grad():
    blabla # 이 조건문 안에서의 연산은 grad 연산이 되지 않는다.
```

# grad 초기화
RNN 사용 때문에 grad는 계속 축적되게 되어있다.    
따라서 매 Update 마다 grad를 초기화 해줘야 한다.  
```
# Option 1, 파라미터 자체의 grad 초기화
w.grad.zero_()
# Option 2, Optimizer 사용시 내부 grad 초기화
optimizer.zero_grad()
```
