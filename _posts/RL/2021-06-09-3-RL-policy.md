---
title: "[RL] 3. Policy"
layout: post
subtitle: Policy
categories:
  - reinforcementlearning
comments: true
---

Policy는 정책으로 Agent가 어떤 행동을 선택하는 지를 결정하는 전략이다.  
Policy는 <img src="https://latex.codecogs.com/gif.latex?\pi" title="\pi" />로 표기한다.  
   
일반적으로 Deterministic한 Policy는 아래와 같이 표기하며,   
어떤 State에서 Agent가 선택하는 행동을 출력한다.   
<img src="https://latex.codecogs.com/gif.latex?\pi(s)&space;=&space;a" title="\pi(s) = a" />
   
만약 Stochastic한 Policy는 아래와 같이 어떤 State에서 어떤 Action이 선택될 확률을 아래와 같이 표기한다.  
<img src="https://latex.codecogs.com/gif.latex?\pi(a&space;|&space;s)" title="\pi(a | s)" />
결국, action이 선택될 확률을 모두 더하면 1이 된다.  
<img src="https://latex.codecogs.com/gif.latex?\sum_{a\in&space;A}^{}\pi(a&space;|&space;s)&space;=&space;1" title="\sum_{a\in A}^{}\pi(a | s) = 1" />
   
---
## Value Function + Policy
이전 포스트에서 Value Function에 대해 설명했다. 하지만 Value Function은 어떤 Policy를 선택하는 지에 따라 달라지게 된다. 따라서 Value function 표기를 아래와 같이 한다.   
**State-value Function**   
<img src="https://latex.codecogs.com/gif.latex?v_{\pi}(s)&space;=&space;E_{\pi}[G_{t}\&space;|\&space;S_{t}=s]" title="v_{\pi}(s) = E_{\pi}[G_{t}\ |\ S_{t}=s]" />
    
**Action-value Function**   
<img src="https://latex.codecogs.com/gif.latex?q_{\pi}(s,&space;a)&space;=&space;E_{\pi}[G_{t}\&space;|\&space;S_{t}=s,&space;A_{t}=a]" title="q_{\pi}(s, a) = E_{\pi}[G_{t}\ |\ S_{t}=s, A_{t}=a]" />
   
---
  
## Optimal Policy
모든 Policy는 항상 한개 이상의 Optimal Policy를 가진다. Optimal Policy는 모든 구간에 대해서 어떤 Policy보다 Value를 높게 가지는 Policy이다.   
Optimal Policy는   
<img src="https://latex.codecogs.com/gif.latex?\pi_{*}" title="\pi_{*}" /> 또는 \*로 표기한다. 
   
따라서 Optimal Policy의 Value Function 표기와 정의는 아래와 같다.
    
**State-value Function**   
<img src="https://latex.codecogs.com/gif.latex?v_{*}(s)&space;=&space;E_{*}[G_{t}\&space;|\&space;S_{t}=s]" title="v_{*}(s) = E_{*}[G_{t}\ |\ S_{t}=s]" />
<img src="https://latex.codecogs.com/gif.latex?=max_{\pi}\&space;v_{\pi}(s)\&space;for\&space;all\&space;s\in&space;S" title="max_{\pi}\ v_{\pi}(s)\ for\ all\ s\in S" />
    
**Action-value Function**   
<img src="https://latex.codecogs.com/gif.latex?q_{*}(s,&space;a)&space;=&space;E_{*}[G_{t}\&space;|\&space;S_{t}=s,&space;A_{t}=a]" title="q_{*}(s, a) = E_{*}[G_{t}\ |\ S_{t}=s, A_{t}=a]" />
<img src="https://latex.codecogs.com/gif.latex?=max_{\pi}\&space;q_{\pi}(s,&space;a)\&space;for\&space;all\&space;s\in&space;S,\&space;a&space;\in&space;A" title="max_{\pi}\ q_{\pi}(s, a)\ for\ all\ s\in S,\ a \in A" />
